{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Map-matching error detection model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from utils import calc_eval_metrics, collate_fn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "proj_dir = \"<YOUR_PROJECT_DIRECTORY>\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available:\", torch.cuda.get_device_name())\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    print(\"GPU is unavailable\")\n",
    "    device = torch.device(\"cuda:1\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LSTM-based model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from lstm_error_detect_model import MatchErrorDetectModel"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(\n",
    "            self, model, train_loader, val_loader, save_dir, criterion=nn.BCEWithLogitsLoss(),\n",
    "            n_epochs=100, patience=1, lr=1e-3, train_batch_size=32, val_batch_size=256, weight_decay=1e-3\n",
    "    ):\n",
    "        self.n_epochs = n_epochs\n",
    "        self.patience = patience\n",
    "\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.val_batch_size = val_batch_size\n",
    "\n",
    "        self.criterion = criterion.to(device)\n",
    "        self.optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        self.lr = lr\n",
    "\n",
    "    def fit(self, model, model_save_prefix, max_kappa=None):\n",
    "        print(\"Training model...\")\n",
    "        model = model.to(device)\n",
    "\n",
    "        # Initialize loss logger\n",
    "        if max_kappa is None:\n",
    "            max_kappa = -float('inf')\n",
    "\n",
    "        loss_log = []\n",
    "        model_save_name_log = []\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            train_loss, train_kappa = model.train_model(\n",
    "                train_loader=self.train_loader, epoch=epoch, train_batch_size=self.train_batch_size,\n",
    "                optimizer=self.optimizer, criterion=self.criterion\n",
    "            )\n",
    "            val_loss, val_kappa = model.eval_model(\n",
    "                val_loader=self.val_loader, eval_batch_size=self.val_batch_size, criterion=self.criterion\n",
    "            )\n",
    "\n",
    "            # Save to log\n",
    "            loss_log.append([train_loss, train_kappa, val_loss, val_kappa])\n",
    "\n",
    "            # Compare to the existing best model\n",
    "            if val_kappa > max_kappa:\n",
    "                # Performance increase\n",
    "                perf_incr = val_kappa - max_kappa\n",
    "\n",
    "                if epoch > self.patience-1 and perf_incr > 0.001:\n",
    "                    max_kappa = val_kappa\n",
    "                    model_save_name = model_save_prefix + f\"_epoch{epoch+1}_trainkappa{train_kappa:.4f}_valkappa{val_kappa:.4f}.pt\"\n",
    "\n",
    "                    # save model\n",
    "                    model_save_path = os.path.join(\n",
    "                        proj_dir, self.save_dir, model_save_name\n",
    "                    )\n",
    "                    torch.save(model, model_save_path)\n",
    "\n",
    "                    print(f\"val_kappa increased to {val_kappa:.4f} at epoch {epoch+1}. Model saved to pt: {model_save_path}\")\n",
    "                    model_save_name_log.append(model_save_name)\n",
    "\n",
    "                else:\n",
    "                    print(f\"val_kappa increased to {val_kappa:.4f} at epoch {epoch+1}. Patience.\")\n",
    "                    model_save_name_log.append(\"no_save\")\n",
    "            else:\n",
    "                model_save_name_log.append(\"no_save\")\n",
    "\n",
    "        loss_log_arr = np.array(loss_log)\n",
    "        loss_log_df = pd.DataFrame(\n",
    "            loss_log_arr, columns=[\"train_loss\", \"train_kappa\", \"val_loss\", \"val_kappa\"]\n",
    "        )\n",
    "        loss_log_df[\"save_name\"] = model_save_name_log\n",
    "        self.loss_log_df = loss_log_df\n",
    "\n",
    "        return loss_log_df\n",
    "\n",
    "    def plot_train_log(self, loss_log_df=None):\n",
    "        if loss_log_df == None:\n",
    "            loss_log_df = self.loss_log_df\n",
    "\n",
    "        f, ax = plt.subplots(1, 2, figsize=(12, 8))\n",
    "        x = np.array(range(len(self.loss_log_df)))\n",
    "\n",
    "        # loss\n",
    "        ax[0].plot(x, loss_log_df['train_loss'], color='dodgerblue', label='Train loss')\n",
    "        ax[0].plot(x, loss_log_df['val_loss'], color='coral', label='Validation loss')\n",
    "\n",
    "\n",
    "        # ade, fde\n",
    "        ax[1].plot(x, loss_log_df['train_kappa'], color='blue', label='Train kappa')\n",
    "        ax[1].plot(x, loss_log_df['val_kappa'], color='red', label='Validation kappa')\n",
    "\n",
    "        ax[0].legend(loc=\"best\")\n",
    "        ax[1].legend(loc=\"best\")\n",
    "\n",
    "        ax[0].set_xlabel(\"Epoch\")\n",
    "        ax[0].set_ylabel(\"Loss\")\n",
    "        ax[1].set_xlabel(\"Epoch\")\n",
    "        ax[1].set_ylabel(\"Kappa\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pre-training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tint = 5\n",
    "\n",
    "# Load pickle\n",
    "pk_name = f\"train_val_test_set_stride20_tint{tint}_ntimes10k_aa.pkl\"\n",
    "with open(\n",
    "    os.path.join(proj_dir, \"Data\", pk_name), 'rb'\n",
    ") as my_file_obj:\n",
    "    train_set, val_set, test_set = pickle.load(my_file_obj)\n",
    "\n",
    "print(f\"Train set size: {len(train_set)}\")\n",
    "print(f\"Val set size: {len(val_set)}\")\n",
    "print(f\"Test set size: {len(test_set)}\")\n",
    "\n",
    "# Training datasets\n",
    "train_batch_size = 32\n",
    "val_batch_size = 1024\n",
    "test_batch_size = 1024\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=train_batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_set, batch_size=val_batch_size, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_set, batch_size=test_batch_size, collate_fn=collate_fn)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "| Dataset             | pos_weight |\n",
    "|---------------------|------------|\n",
    "| AA synthetic (5 s)  | 3.9606     |\n",
    "| AA synthetic (10 s) | 4.2149     |\n",
    "| AA synthetic (30 s) | 5.8076     |\n",
    "| eVED real (5s)      | 5.2330     |"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Match error detection model\n",
    "input_dim = 4\n",
    "hidden_size = 1024\n",
    "n_enc_layers = 1\n",
    "edge_embed_dim = 4\n",
    "n_dec_layers = 1\n",
    "dropout = 0.2\n",
    "\n",
    "error_clf = MatchErrorDetectModel(\n",
    "    input_dim=input_dim, hidden_size=hidden_size, edge_embed_dim=edge_embed_dim,\n",
    "    n_enc_layers=n_enc_layers, n_dec_layers=n_dec_layers, dropout=dropout\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "save_dir = \"<YOUR_MODEL_SAVE_DIRECTION>\"\n",
    "pos_weight = torch.tensor(3.9606)\n",
    "trainer = Trainer(\n",
    "    model=error_clf, train_loader=train_loader, val_loader=val_loader, save_dir=save_dir,\n",
    "    criterion=nn.BCEWithLogitsLoss(pos_weight=pos_weight), n_epochs=100, patience=1, lr=1e-4,\n",
    "    train_batch_size=train_batch_size, val_batch_size=val_batch_size, weight_decay=1e-3\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "model_save_prefix = f\"trajedge_lstm_tint{tint}_hidden{hidden_size}_nlayers{n_enc_layers}_dp02\"\n",
    "loss_log_df = trainer.fit(error_clf, model_save_prefix)\n",
    "\n",
    "# save to pickle\n",
    "pk_name = f\"losslogdf_trajedge_lstm_tint{tint}_hidden{hidden_size}_nlayers{n_enc_layers}_dp02.pkl\"\n",
    "with open(os.path.join(proj_dir, save_dir, pk_name), 'wb') as my_file_obj:\n",
    "    pickle.dump(loss_log_df, my_file_obj)\n",
    "\n",
    "# print the best model info\n",
    "loss_min_idx = loss_log_df['val_kappa'].idxmax()\n",
    "print(\"\\nBest model:\", loss_log_df.loc[loss_min_idx, :])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load best model\n",
    "save_dir = \"<YOUR_MODEL_SAVE_DIRECTION>\"\n",
    "model_save_name = \"<YOUR_BEST_MODEL>\"\n",
    "model_save_path = os.path.join(proj_dir, save_dir, model_save_name)\n",
    "error_clf = torch.load(model_save_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tint = 5\n",
    "\n",
    "# Load pickle\n",
    "pk_name = f\"train_val_test_set_stride20_tint{tint}_ntimes10k_aa.pkl\"\n",
    "with open(\n",
    "        os.path.join(proj_dir, \"Data\", pk_name), 'rb'\n",
    ") as my_file_obj:\n",
    "    train_set, val_set, test_set = pickle.load(my_file_obj)\n",
    "\n",
    "print(f\"Train set size: {len(train_set)}\")\n",
    "print(f\"Val set size: {len(val_set)}\")\n",
    "print(f\"Test set size: {len(test_set)}\")\n",
    "\n",
    "# Training datasets\n",
    "train_batch_size = 32\n",
    "val_batch_size = 1024\n",
    "test_batch_size = 1024\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=train_batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_set, batch_size=val_batch_size, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_set, batch_size=test_batch_size, collate_fn=collate_fn)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "pos_weight = torch.tensor(3.9606)\n",
    "val_loss, val_kappa = error_clf.eval_model(test_loader, test_batch_size, criterion=nn.BCEWithLogitsLoss(pos_weight=pos_weight))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fine-tuning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fine-tuning is performed multiple times on various datasets. Below is an example using the real trajectory dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load best model\n",
    "save_dir = \"<YOUR_MODEL_SAVE_DIRECTION>\"\n",
    "model_save_name = \"<YOUR_BEST_MODEL>\"\n",
    "model_save_path = os.path.join(proj_dir, save_dir, model_save_name)\n",
    "error_clf = torch.load(model_save_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load pickle\n",
    "pk_name = \"train_val_test_set_stride2_labeled184.pkl\"\n",
    "with open(\n",
    "        os.path.join(proj_dir, \"Data\", pk_name), 'rb'\n",
    ") as my_file_obj:\n",
    "    train_set, val_set, test_set = pickle.load(my_file_obj)\n",
    "\n",
    "print(f\"Train set size: {len(train_set)}\")\n",
    "print(f\"Val set size: {len(val_set)}\")\n",
    "print(f\"Test set size: {len(test_set)}\")\n",
    "\n",
    "# Training datasets\n",
    "train_batch_size = 32\n",
    "val_batch_size = 128\n",
    "test_batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=train_batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_set, batch_size=val_batch_size, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_set, batch_size=test_batch_size, collate_fn=collate_fn)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hidden_size = 1024\n",
    "n_enc_layers = 1\n",
    "\n",
    "# Trainer\n",
    "save_dir = \"<YOUR_MODEL_SAVE_DIRECTION>\"\n",
    "pos_weight = torch.tensor(5.233)\n",
    "trainer = Trainer(\n",
    "    model=error_clf, train_loader=train_loader, val_loader=val_loader, save_dir=save_dir,\n",
    "    criterion=nn.BCEWithLogitsLoss(pos_weight=pos_weight), n_epochs=50, patience=1, lr=1e-4,\n",
    "    train_batch_size=train_batch_size, val_batch_size=val_batch_size, weight_decay=1e-3\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "model_save_prefix = f\"syn5real_trajedge_lstm_hidden{hidden_size}_nlayers{n_enc_layers}_dp02\"\n",
    "loss_log_df = trainer.fit(error_clf, model_save_prefix)\n",
    "\n",
    "# Save to pickle\n",
    "pk_name = f\"losslogdf_syn5real_trajedge_lstm_hidden{hidden_size}_nlayers{n_enc_layers}_dp02.pkl\"\n",
    "with open(os.path.join(proj_dir, save_dir, pk_name), 'wb') as my_file_obj:\n",
    "    pickle.dump(loss_log_df, my_file_obj)\n",
    "\n",
    "# Print the best model info\n",
    "loss_min_idx = loss_log_df['val_kappa'].idxmax()\n",
    "print(\"\\nBest model:\", loss_log_df.loc[loss_min_idx, :])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transformer-based model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformer_error_detect_model import MatchErrorDetectModel"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(\n",
    "            self, model, train_loader, val_loader, save_dir, criterion=nn.BCEWithLogitsLoss(),\n",
    "            n_epochs=100, patience=1, lr=1e-3, train_batch_size=32, val_batch_size=256, weight_decay=1e-3\n",
    "    ):\n",
    "        self.n_epochs = n_epochs\n",
    "        self.patience = patience\n",
    "\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.val_batch_size = val_batch_size\n",
    "\n",
    "        self.criterion = criterion.to(device)\n",
    "        self.optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        self.lr = lr\n",
    "\n",
    "    def fit(self, model, model_save_prefix, max_kappa=None):\n",
    "        print(\"Training model...\")\n",
    "        model = model.to(device)\n",
    "\n",
    "        # Initialize loss logger\n",
    "        if max_kappa is None:\n",
    "            max_kappa = -float('inf')\n",
    "\n",
    "        loss_log = []\n",
    "        model_save_name_log = []\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            train_loss, train_kappa = model.train_model(\n",
    "                train_loader=self.train_loader, epoch=epoch, train_batch_size=self.train_batch_size,\n",
    "                optimizer=self.optimizer, criterion=self.criterion\n",
    "            )\n",
    "            val_loss, val_kappa = model.eval_model(\n",
    "                val_loader=self.val_loader, eval_batch_size=self.val_batch_size, criterion=self.criterion\n",
    "            )\n",
    "\n",
    "            # Save to log\n",
    "            loss_log.append([train_loss, train_kappa, val_loss, val_kappa])\n",
    "\n",
    "            # Compare to the existing best model\n",
    "            if val_kappa > max_kappa:\n",
    "                # Performance increase\n",
    "                perf_incr = val_kappa - max_kappa\n",
    "\n",
    "                if epoch > self.patience-1 and perf_incr > 0.001:\n",
    "                    max_kappa = val_kappa\n",
    "                    model_save_name = model_save_prefix + f\"_epoch{epoch+1}_trainkappa{train_kappa:.4f}_valkappa{val_kappa:.4f}.pt\"\n",
    "\n",
    "                    # save model\n",
    "                    model_save_path = os.path.join(\n",
    "                        proj_dir, self.save_dir, model_save_name\n",
    "                    )\n",
    "                    torch.save(model, model_save_path)\n",
    "\n",
    "                    print(f\"val_kappa increased to {val_kappa:.4f} at epoch {epoch+1}. Model saved to pt: {model_save_path}\")\n",
    "                    model_save_name_log.append(model_save_name)\n",
    "\n",
    "                else:\n",
    "                    print(f\"val_kappa increased to {val_kappa:.4f} at epoch {epoch+1}. Patience.\")\n",
    "                    model_save_name_log.append(\"no_save\")\n",
    "            else:\n",
    "                model_save_name_log.append(\"no_save\")\n",
    "\n",
    "        loss_log_arr = np.array(loss_log)\n",
    "        loss_log_df = pd.DataFrame(\n",
    "            loss_log_arr, columns=[\"train_loss\", \"train_kappa\", \"val_loss\", \"val_kappa\"]\n",
    "        )\n",
    "        loss_log_df[\"save_name\"] = model_save_name_log\n",
    "        self.loss_log_df = loss_log_df\n",
    "\n",
    "        return loss_log_df\n",
    "\n",
    "    def plot_train_log(self, loss_log_df=None):\n",
    "        if loss_log_df == None:\n",
    "            loss_log_df = self.loss_log_df\n",
    "\n",
    "        f, ax = plt.subplots(1, 2, figsize=(12, 8))\n",
    "        x = np.array(range(len(self.loss_log_df)))\n",
    "\n",
    "        # loss\n",
    "        ax[0].plot(x, loss_log_df['train_loss'], color='dodgerblue', label='Train loss')\n",
    "        ax[0].plot(x, loss_log_df['val_loss'], color='coral', label='Validation loss')\n",
    "\n",
    "\n",
    "        # ade, fde\n",
    "        ax[1].plot(x, loss_log_df['train_kappa'], color='blue', label='Train kappa')\n",
    "        ax[1].plot(x, loss_log_df['val_kappa'], color='red', label='Validation kappa')\n",
    "\n",
    "        ax[0].legend(loc=\"best\")\n",
    "        ax[1].legend(loc=\"best\")\n",
    "\n",
    "        ax[0].set_xlabel(\"Epoch\")\n",
    "        ax[0].set_ylabel(\"Loss\")\n",
    "        ax[1].set_xlabel(\"Epoch\")\n",
    "        ax[1].set_ylabel(\"Kappa\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load pickle\n",
    "pk_name = \"train_val_test_set_stride2_labeled184.pkl\"\n",
    "with open(\n",
    "        os.path.join(proj_dir, \"Data\", pk_name), 'rb'\n",
    ") as my_file_obj:\n",
    "    train_set, val_set, test_set = pickle.load(my_file_obj)\n",
    "\n",
    "print(f\"Train set size: {len(train_set)}\")\n",
    "print(f\"Val set size: {len(val_set)}\")\n",
    "print(f\"Test set size: {len(test_set)}\")\n",
    "\n",
    "# Training datasets\n",
    "train_batch_size = 32\n",
    "val_batch_size = 1024\n",
    "test_batch_size = 1024\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=train_batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_set, batch_size=val_batch_size, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_set, batch_size=test_batch_size, collate_fn=collate_fn)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Match error detection model\n",
    "input_dim = 4\n",
    "n_head = 2\n",
    "edge_embed_dim = 4\n",
    "n_enc_layers = 2\n",
    "n_dec_layers = 2\n",
    "dropout = 0.2\n",
    "\n",
    "d_model_ls = [16, 32, 64]\n",
    "d_hid_ls = [512, 1024, 2048, 4096]\n",
    "\n",
    "for d_model in d_model_ls:\n",
    "    for d_hid in d_hid_ls:\n",
    "        print(f\"Now experimenting with d_model: {d_model}, d_hid: {d_hid}\")\n",
    "\n",
    "        error_clf = MatchErrorDetectModel(\n",
    "            input_dim=input_dim, d_model=d_model, n_head=n_head, d_hid=d_hid, edge_embed_dim=edge_embed_dim,\n",
    "            n_enc_layers=n_enc_layers, n_dec_layers=n_dec_layers, dropout=dropout\n",
    "        )\n",
    "\n",
    "        # Trainer\n",
    "        save_dir = \"<YOUR_MODEL_SAVE_DIRECTION>\"\n",
    "        pos_weight = torch.tensor(5.233)\n",
    "        trainer = Trainer(\n",
    "            model=error_clf, train_loader=train_loader, val_loader=val_loader, save_dir=save_dir,\n",
    "            criterion=nn.BCEWithLogitsLoss(pos_weight=pos_weight), n_epochs=20, patience=1, lr=1e-3,\n",
    "            train_batch_size=train_batch_size, val_batch_size=val_batch_size, weight_decay=1e-3\n",
    "        )\n",
    "\n",
    "        # Fit the model\n",
    "        model_save_prefix = f\"trajedge_trans_dmodel{d_model}_nhead{n_head}_dhid{d_hid}_nlayers{n_enc_layers}_dp02\"\n",
    "        loss_log_df = trainer.fit(error_clf, model_save_prefix)\n",
    "\n",
    "        # save to pickle\n",
    "        pk_name = f\"losslogdf_trajedge_trans_dmodel{d_model}_nhead{n_head}_dhid{d_hid}_nlayers{n_enc_layers}_dp02.pkl\"\n",
    "        with open(os.path.join(proj_dir, save_dir, pk_name), 'wb') as my_file_obj:\n",
    "            pickle.dump(loss_log_df, my_file_obj)\n",
    "\n",
    "        # print the best model info\n",
    "        loss_min_idx = loss_log_df['val_kappa'].idxmax()\n",
    "        print(\"\\nBest model:\", loss_log_df.loc[loss_min_idx, :])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prediction (error detection) using the LSTM-based model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load best model\n",
    "save_dir = \"Data\"\n",
    "model_save_name = \"syn5real_trajedge_lstm_hidden1024_nlayers1_dp02_epoch47_trainkappa1.0000_valkappa0.9847.pt\"\n",
    "model_save_path = os.path.join(proj_dir, save_dir, model_save_name)\n",
    "error_clf = torch.load(model_save_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Predictions on the test set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load pickle\n",
    "pk_name = \"train_val_test_set_stride2_labeled184.pkl\"\n",
    "with open(\n",
    "        os.path.join(proj_dir, \"Data\", pk_name), 'rb'\n",
    ") as my_file_obj:\n",
    "    train_set, val_set, test_set = pickle.load(my_file_obj)\n",
    "\n",
    "print(\"eVED real\")\n",
    "print(f\"Train set size: {len(train_set)}\")\n",
    "print(f\"Val set size: {len(val_set)}\")\n",
    "print(f\"Test set size: {len(test_set)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "probs_ls = []\n",
    "pred_labels_ls = []\n",
    "\n",
    "for sample_i in trange(len(test_set)):\n",
    "    # Sample\n",
    "    single_sample = test_set[sample_i]\n",
    "    src = torch.unsqueeze(single_sample[0], 0).to(device)\n",
    "    src_lengths = torch.tensor([src.shape[1]], dtype=torch.int64)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Prediction\n",
    "        probs, pred_labels = error_clf.predict(src=src, src_lengths=src_lengths)\n",
    "\n",
    "        # Convert to np.arr on CPU\n",
    "        probs_arr = probs.cpu().numpy()[0]\n",
    "        pred_labels_arr = pred_labels.cpu().numpy()[0]\n",
    "\n",
    "    # Save to ls\n",
    "    probs_ls.append(probs_arr)\n",
    "    pred_labels_ls.append(pred_labels_arr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Predictions on all samples"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load pickle\n",
    "pk_name = \"xy_ls_stride2_labeled184.pkl\"\n",
    "with open(os.path.join(proj_dir, \"Data\", pk_name), 'rb'\n",
    ") as my_file_obj:\n",
    "    x_ls, y_ls = pickle.load(my_file_obj)\n",
    "\n",
    "print(f\"x_ls len: {len(x_ls)}, y_ls len: {len(y_ls)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# PyTorch dataset\n",
    "all_sample_set = MatchErrorDataset(x_ls, y_ls)\n",
    "\n",
    "probs_ls = []\n",
    "pred_labels_ls = []\n",
    "\n",
    "for sample_i in trange(len(all_sample_set)):\n",
    "    # Sample\n",
    "    single_sample = all_sample_set[sample_i]\n",
    "    src = torch.unsqueeze(single_sample[0], 0).to(device)\n",
    "    src_lengths = torch.tensor([src.shape[1]], dtype=torch.int64)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Prediction\n",
    "        probs, pred_labels = error_clf.predict(src=src, src_lengths=src_lengths)\n",
    "\n",
    "        # Convert to np.arr on CPU\n",
    "        probs_arr = probs.cpu().numpy()[0]\n",
    "        pred_labels_arr = pred_labels.cpu().numpy()[0]\n",
    "\n",
    "    # Save to ls\n",
    "    probs_ls.append(probs_arr)\n",
    "    pred_labels_ls.append(pred_labels_arr)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
