{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Create training samples for the map-matching error detection model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import osmnx as ox\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, LineString\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from utils import cal_d_hdg, normalize, create_samples, calc_pos_weight, organize_trajs\n",
    "from utils import tensorize_input, MatchErrorDataset, train_val_test_set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "proj_dir = \"<YOUR_PROJECT_DIRECTORY>\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Real samples"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load pickle\n",
    "pk_name = \"labeled184_pts_traj_ptid.pkl\"\n",
    "with open(\n",
    "    os.path.join(proj_dir, \"Data\", pk_name), 'rb'\n",
    ") as my_file_obj:\n",
    "    pts_gdf, traj_ptid_ls = pickle.load(my_file_obj)\n",
    "\n",
    "print(\"Number of trajectories:\", len(traj_ptid_ls))\n",
    "print(\"Number of points:\", len(pts_gdf))\n",
    "pts_gdf.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Map-matching error statistics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_pts = len(pts_gdf)\n",
    "n_ones = sum(pts_gdf[\"mat_error\"])\n",
    "n_zeros = n_pts - n_ones\n",
    "\n",
    "labeled_stats_df = pd.DataFrame(\n",
    "    {\n",
    "        \"correct\": n_zeros, \"wrong\": n_ones, \"n_pts\": n_pts,\n",
    "        \"error_rate\": round(n_ones / n_pts, 4)\n",
    "    }, index=[0]\n",
    ")\n",
    "\n",
    "labeled_stats_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Road network attributes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load road network graph\n",
    "graph_name = \"aa_road_graph_drive_service_bbox_time_speed_bearing.graphml\"  # Ann Arbor\n",
    "roadnet_graph = ox.load_graphml(\n",
    "    os.path.join(proj_dir, graph_name)\n",
    ")\n",
    "print(\"Road graph loaded!\")\n",
    "\n",
    "# Graph to gdf\n",
    "nodes_gdf, edges_gdf = ox.graph_to_gdfs(roadnet_graph)\n",
    "print(\"nodes_gdf, edges_gdf created!\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Some roads (edges) have more than one class in the \"highway\" field, we choose its first class that is not \"unclassified\"."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "edges_gdf[\"edge_class\"] = copy.deepcopy(edges_gdf[\"highway\"])\n",
    "\n",
    "for edge_i in trange(len(edges_gdf)):\n",
    "    edge_i_class = edges_gdf.loc[edge_i, \"edge_class\"]\n",
    "    if isinstance(edge_i_class, list):\n",
    "        class_j = \"unclassified\"  # Initialize class_j\n",
    "        for class_j in edge_i_class:\n",
    "            if class_j == \"unclassified\":\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        edges_gdf.loc[edge_i, \"edge_class\"] = class_j"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Merge \"services\" with \"service\" in the edge class."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Rename services to service\n",
    "edges_gdf.loc[edges_gdf[\"edge_class\"] == \"services\", \"edge_class\"] = \"service\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Overview of all the edges\n",
    "edges_gdf.groupby(by=[\"edge_class\"]).count()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Point attributes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Associate each matched tracking point to its closest edge."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Spatial index\n",
    "spatial_idx = edges_gdf.sindex\n",
    "\n",
    "closest_edges_ls = []\n",
    "for pt_i in trange(len(pts_gdf)):\n",
    "    # Create a buffer to find possible matched edges\n",
    "    buffer_dist = 1000\n",
    "    pt_i_buffer = pts_gdf.loc[pt_i, \"geometry\"].buffer(buffer_dist)\n",
    "    possible_matches_idx_ls = list(spatial_idx.intersection(pt_i_buffer.bounds))\n",
    "\n",
    "    # Retrieve the possible matched edges from the GeoDataFrame\n",
    "    possible_matches_gdf = edges_gdf.iloc[possible_matches_idx_ls]\n",
    "\n",
    "    # Calculate distances from the point to these edges\n",
    "    distances = possible_matches_gdf.distance(pts_gdf.iloc[pt_i][\"geometry\"])\n",
    "\n",
    "    # Find the closest edge\n",
    "    closest_edge_idx = distances.idxmin()\n",
    "    closest_edges_ls.append((pt_i, closest_edge_idx, distances[closest_edge_idx]))\n",
    "\n",
    "# Convert the list of closest edges to a DataFrame for easier handling\n",
    "closest_edges_df = pd.DataFrame(closest_edges_ls, columns=['pt_idx', 'closest_edge_idx', 'distance'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Difference between two consecutive headings\n",
    "pts_gdf[\"d_hdg\"] = 0.0  # Delta heading\n",
    "# Difference between the heading and road bearing\n",
    "pts_gdf[\"d_hdg_brg\"] = 0.0  # Delta heading-bearing\n",
    "# Distance to the closest road\n",
    "pts_gdf[\"dist2road\"] = 1000.0\n",
    "# Edge class\n",
    "pts_gdf[\"edge_class\"] = \"unclassified\"\n",
    "\n",
    "for traj_i in tqdm(traj_ptid_ls):\n",
    "    for pt_count in range(len(traj_i[1])):\n",
    "        pt_j = traj_i[1][pt_count]\n",
    "        # Delta heading\n",
    "        if pt_count == 0:  # 1st pt in a trajectory\n",
    "            pts_gdf.loc[pt_j, \"d_hdg\"] = 0\n",
    "        else:\n",
    "            pt_j_prev1 = traj_i[1][pt_count-1]  # Previous pt\n",
    "            pts_gdf.loc[pt_j, \"d_hdg\"] = cal_d_hdg(\n",
    "                hdg1=pts_gdf.loc[pt_j, \"heading\"], hdg2=pts_gdf.loc[pt_j_prev1, \"heading\"]\n",
    "            )\n",
    "        # Delta heading-bearing\n",
    "        closest_edge_idx = closest_edges_df.loc[pt_j, \"closest_edge_idx\"]\n",
    "        pts_gdf.loc[pt_j, \"d_hdg_brg\"] = cal_d_hdg(\n",
    "            hdg1=pts_gdf.loc[pt_j, \"heading\"], hdg2=edges_gdf.loc[closest_edge_idx, \"bearing\"]\n",
    "        )\n",
    "        # Distance to the closest road\n",
    "        pts_gdf.loc[pt_j, \"dist2road\"] = closest_edges_df.loc[pt_j, \"distance\"]\n",
    "        # Edge class\n",
    "        pts_gdf.loc[pt_j, \"edge_class\"] = edges_gdf.loc[closest_edge_idx, \"edge_class\"]\n",
    "\n",
    "# Fill nan with 90\n",
    "pts_gdf = pts_gdf.fillna(90)\n",
    "# Double check nan\n",
    "nan_row_ids = pts_gdf.isna().any(axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Convert `pts_gdf[\"edge_class\"]` from categorical strings to encoding values."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pts_gdf['edge_class_encoded'] = pd.Categorical(\n",
    "    pts_gdf['edge_class'],\n",
    "    categories=[\n",
    "        \"living_street\", \"motorway\", \"motorway_link\", \"primary\", \"primary_link\", \"residential\",\n",
    "        \"secondary\", \"secondary_link\", \"service\", \"tertiary\", \"tertiary_link\", \"trunk\", \"unclassified\"\n",
    "    ]\n",
    ").codes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Normalization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "columns2norm_ls = [\"d_hdg\", \"d_hdg_brg\", \"dist2road\"]\n",
    "target_range_tp_ls = [(-180, 180), (-180, 180), (0, 500)]\n",
    "\n",
    "for col_count, col_i in enumerate(columns2norm_ls):\n",
    "    print(\"Now normalizing column:\", col_i)\n",
    "    x_arr = pts_gdf[col_i].to_numpy()\n",
    "    x_norm_arr = normalize(x_arr, orig_range_tp=target_range_tp_ls[col_count], target_range_tp=(0.1, 1))\n",
    "\n",
    "    # Add normalized values back to pts_df\n",
    "    new_col_name = col_i + \"_norm\"\n",
    "    pts_gdf[new_col_name] = x_norm_arr\n",
    "\n",
    "    print(f\"Range after normalization: ({round(min(x_norm_arr), 3)}, {round(max(x_norm_arr), 3)})\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Samples"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "samp_pts_ls = create_samples(\n",
    "    min_seq_len=10, max_seq_len=100, stride=2, traj_ptid_ls=traj_ptid_ls\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_ls = []\n",
    "y_ls = []\n",
    "\n",
    "for sample_i in tqdm(samp_pts_ls):\n",
    "    # Sample input\n",
    "    x_i = pts_gdf.loc[sample_i, [\"d_hdg\", \"d_hdg_brg\", \"dist2road\", \"edge_class_encoded\"]].to_numpy()\n",
    "    # Sample output\n",
    "    y_i = pts_gdf.loc[sample_i, \"mat_error\"].to_numpy()\n",
    "\n",
    "    x_ls.append(x_i)\n",
    "    y_ls.append(y_i)\n",
    "\n",
    "print(f\"x_ls len: {len(x_ls)}, y_ls len: {len(y_ls)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Calculate the positive weight pos_weight, a parameter needed in the weighted cross-entropy loss."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "weight_for_class_1 = round(calc_pos_weight(y_ls), 4)\n",
    "weight_for_class_1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Training, validation, and test sets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = train_val_test_set(x_ls, y_ls, train_pct=0.7)\n",
    "\n",
    "print(f\"Train set size: {len(train_set)}\")\n",
    "print(f\"Val set size: {len(val_set)}\")\n",
    "print(f\"Test set size: {len(test_set)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save to pickle\n",
    "pk_name = \"train_val_test_set_stride2_labeled184.pkl\"\n",
    "with open(os.path.join(proj_dir, pk_name), 'wb') as my_file_obj:\n",
    "    pickle.dump([train_set, val_set, test_set], my_file_obj)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Synthetic samples"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Time interval\n",
    "tint = 5\n",
    "\n",
    "# Load pickle\n",
    "pk_name = f\"track_pts_tint{tint}_ntimes10k_fixeddist20_error02.pkl\"\n",
    "with open(os.path.join(proj_dir, pk_name), 'rb') as my_file_obj:\n",
    "    pts_gdf = pickle.load(my_file_obj)\n",
    "\n",
    "traj_ptid_ls = organize_trajs(pts_df=pts_gdf, use_traj_id=True)\n",
    "\n",
    "print(\"Number of trajectories:\", len(traj_ptid_ls))\n",
    "print(\"Number of points:\", len(pts_gdf))\n",
    "pts_gdf.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Road network attributes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load road network graph\n",
    "graph_name = \"aa_road_graph_drive_service_bbox_time_speed_bearing.graphml\"  # Ann Arbor\n",
    "# graph_name = \"la_road_graph_drive_service_bbox_time_speed_bearing.graphml\"  # Los Angeles\n",
    "roadnet_graph = ox.load_graphml(\n",
    "    os.path.join(proj_dir, graph_name)\n",
    ")\n",
    "print(\"Road graph loaded!\")\n",
    "\n",
    "# Graph to gdf\n",
    "nodes_gdf, edges_gdf = ox.graph_to_gdfs(roadnet_graph)\n",
    "print(\"nodes_gdf, edges_gdf created!\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Some roads (edges) have more than one class in the \"highway\" field, we choose its first class that is not \"unclassified\"."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "edges_gdf[\"edge_class\"] = copy.deepcopy(edges_gdf[\"highway\"])\n",
    "\n",
    "for edge_i in trange(len(edges_gdf)):\n",
    "    edge_i_class = edges_gdf.loc[edge_i, \"edge_class\"]\n",
    "    if isinstance(edge_i_class, list):\n",
    "        class_j = \"unclassified\"  # Initialize class_j\n",
    "        for class_j in edge_i_class:\n",
    "            if class_j == \"unclassified\":\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        edges_gdf.loc[edge_i, \"edge_class\"] = class_j"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Merge \"services\" with \"service\" in the edge class."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Rename services to service\n",
    "edges_gdf.loc[edges_gdf[\"edge_class\"] == \"services\", \"edge_class\"] = \"service\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Overview of all the edges\n",
    "edges_gdf.groupby(by=[\"edge_class\"]).count()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Point attributes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Spatial index\n",
    "spatial_idx = edges_gdf.sindex\n",
    "\n",
    "closest_edges_ls = []\n",
    "for pt_i in trange(len(pts_gdf)):\n",
    "    # Create a buffer to find possible matched edges\n",
    "    buffer_dist = 1000\n",
    "    pt_i_buffer = pts_gdf.loc[pt_i, \"geometry\"].buffer(buffer_dist)\n",
    "    possible_matches_idx_ls = list(spatial_idx.intersection(pt_i_buffer.bounds))\n",
    "\n",
    "    # Retrieve the possible matched edges from the GeoDataFrame\n",
    "    possible_matches_gdf = edges_gdf.iloc[possible_matches_idx_ls]\n",
    "\n",
    "    # Calculate distances from the point to these edges\n",
    "    distances = possible_matches_gdf.distance(pts_gdf.iloc[pt_i][\"geometry\"])\n",
    "\n",
    "    # Find the closest edge\n",
    "    closest_edge_idx = distances.idxmin()\n",
    "    closest_edges_ls.append((pt_i, closest_edge_idx, distances[closest_edge_idx]))\n",
    "\n",
    "# Convert the list of closest edges to a DataFrame for easier handling\n",
    "closest_edges_df = pd.DataFrame(closest_edges_ls, columns=['pt_idx', 'closest_edge_idx', 'distance'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Difference between two consecutive headings\n",
    "pts_gdf[\"d_hdg\"] = 0.0  # Delta heading\n",
    "# Difference between the heading and road bearing\n",
    "pts_gdf[\"d_hdg_brg\"] = 0.0  # Delta heading-bearing\n",
    "# Distance to the closest road\n",
    "pts_gdf[\"dist2road\"] = 1000.0\n",
    "# Edge class\n",
    "pts_gdf[\"edge_class\"] = \"unclassified\"\n",
    "\n",
    "for traj_i in tqdm(traj_ptid_ls):\n",
    "    for pt_count in range(len(traj_i[1])):\n",
    "        pt_j = traj_i[1][pt_count]\n",
    "        # Delta heading\n",
    "        if pt_count == 0:  # 1st pt in a trajectory\n",
    "            pts_gdf.loc[pt_j, \"d_hdg\"] = 0\n",
    "        else:\n",
    "            pt_j_prev1 = traj_i[1][pt_count-1]  # Previous pt\n",
    "            pts_gdf.loc[pt_j, \"d_hdg\"] = cal_d_hdg(\n",
    "                hdg1=pts_gdf.loc[pt_j, \"heading\"], hdg2=pts_gdf.loc[pt_j_prev1, \"heading\"]\n",
    "            )\n",
    "        # Delta heading-bearing\n",
    "        closest_edge_idx = closest_edges_df.loc[pt_j, \"closest_edge_idx\"]\n",
    "        pts_gdf.loc[pt_j, \"d_hdg_brg\"] = cal_d_hdg(\n",
    "            hdg1=pts_gdf.loc[pt_j, \"heading\"], hdg2=edges_gdf.loc[closest_edge_idx, \"bearing\"]\n",
    "        )\n",
    "        # Distance to the closest road\n",
    "        pts_gdf.loc[pt_j, \"dist2road\"] = closest_edges_df.loc[pt_j, \"distance\"]\n",
    "        # Edge class\n",
    "        pts_gdf.loc[pt_j, \"edge_class\"] = edges_gdf.loc[closest_edge_idx, \"edge_class\"]\n",
    "\n",
    "# Fill nan with 90\n",
    "pts_gdf = pts_gdf.fillna(90)\n",
    "# Double check nan\n",
    "nan_row_ids = pts_gdf.isna().any(axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pts_gdf['edge_class_encoded'] = pd.Categorical(\n",
    "    pts_gdf['edge_class'],\n",
    "    categories=[\n",
    "        \"living_street\", \"motorway\", \"motorway_link\", \"primary\", \"primary_link\", \"residential\",\n",
    "        \"secondary\", \"secondary_link\", \"service\", \"tertiary\", \"tertiary_link\", \"trunk\", \"unclassified\"\n",
    "    ]\n",
    ").codes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Normalization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "columns2norm_ls = [\"d_hdg\", \"d_hdg_brg\", \"dist2road\"]\n",
    "target_range_tp_ls = [(-180, 180), (-180, 180), (0, 500)]\n",
    "\n",
    "for col_count, col_i in enumerate(columns2norm_ls):\n",
    "    print(\"Now normalizing column:\", col_i)\n",
    "    x_arr = pts_gdf[col_i].to_numpy()\n",
    "    x_norm_arr = normalize(x_arr, orig_range_tp=target_range_tp_ls[col_count], target_range_tp=(0.1, 1))\n",
    "\n",
    "    # Add normalized values back to pts_df\n",
    "    new_col_name = col_i + \"_norm\"\n",
    "    pts_gdf[new_col_name] = x_norm_arr\n",
    "\n",
    "    print(f\"Range after normalization: ({round(min(x_norm_arr), 3)}, {round(max(x_norm_arr), 3)})\")\n",
    "\n",
    "# Map original labels 0, 1, 2 to 0, 1\n",
    "pts_gdf[\"error01\"] = (pts_gdf[\"mat_error\"] > 0).astype(\"int\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Samples"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "samp_pts_ls = create_samples(\n",
    "    min_seq_len=10, max_seq_len=100, stride=20, traj_ptid_ls=traj_ptid_ls\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_ls = []\n",
    "y_ls = []\n",
    "\n",
    "for sample_i in tqdm(samp_pts_ls):\n",
    "    # Sample input\n",
    "    x_i = pts_gdf.loc[sample_i, [\"d_hdg\", \"d_hdg_brg\", \"dist2road\", \"edge_class_encoded\"]].to_numpy()\n",
    "    # Sample output\n",
    "    y_i = pts_gdf.loc[sample_i, \"error01\"].to_numpy()\n",
    "\n",
    "    x_ls.append(x_i)\n",
    "    y_ls.append(y_i)\n",
    "\n",
    "print(f\"x_ls len: {len(x_ls)}, y_ls len: {len(y_ls)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Calculate the positive weight pos_weight, a parameter needed in the weighted cross-entropy loss."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "weight_for_class_1 = round(calc_pos_weight(y_ls), 4)\n",
    "weight_for_class_1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Training, validation, and test sets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = train_val_test_set(x_ls, y_ls, train_pct=0.7)\n",
    "\n",
    "print(f\"Train set size: {len(train_set)}\")\n",
    "print(f\"Val set size: {len(val_set)}\")\n",
    "print(f\"Test set size: {len(test_set)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save to pickle\n",
    "pk_name = f\"train_val_test_set_stride20_tint{tint}_ntimes10k.pkl\"\n",
    "with open(\n",
    "        os.path.join(proj_dir, pk_name), 'wb'\n",
    ") as my_file_obj:\n",
    "    pickle.dump([train_set, val_set, test_set], my_file_obj)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
